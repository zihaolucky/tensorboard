{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Understanding the vanishing gradient problem through visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There're reasons why deep neural network could work very well, while few people get a promising result or make it possible by simply make their neural network *deep*.\n",
    "\n",
    "* Computational power and data grow tremendously. People need more complex model and faster computer to make it feasible.\n",
    "* Realize and understand the difficulties associated with training a deep model.\n",
    "\n",
    "In this tutorial, we would like to show you some insights of the techniques that researchers find useful in training a deep model, using MXNet and its visualizing tool -- TensorBoard.\n",
    "\n",
    "Let’s recap some of the relevant issues on training a deep model:\n",
    "\n",
    "* Weight initialization.  If you initialize the network with random and small weights, when you look at the gradients down the top layer, you would find they’re getting smaller and smaller, then the first layer almost doesn’t change as the gradients are too small to make a significant update. Without a chance to learn the first layer effectively, it's impossible to update and learn a good deep model.\n",
    "* Nonlinearity activation. When people use `sigmoid` or `tanh` as activation function, the gradient, same as the above, is getting smaller and smaller. Just remind the formula of the parameter updates and the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setting\n",
    "\n",
    "Here we create a simple MLP for cifar10 dataset and visualize the learning processing through loss/accuracy, and its gradient distributions, by changing its initialization and activation setting.\n",
    "\n",
    "## General Setting \n",
    "\n",
    "We adopt MLP as our model and run our experiment in MNIST dataset. Then we'll visualize the weight and gradient of a layer using `Monitor` in MXNet and `Histogram` in TensorBoard.\n",
    "\n",
    "### Network Structure\n",
    "\n",
    "Here's the network structure:\n",
    "\n",
    "```python\n",
    "def get_mlp(acti=\"relu\"):\n",
    "    \"\"\"\n",
    "    multi-layer perceptron\n",
    "    \"\"\"\n",
    "    data = mx.symbol.Variable('data')\n",
    "    fc   = mx.symbol.FullyConnected(data = data, name='fc', num_hidden=512)\n",
    "    act  = mx.symbol.Activation(data = fc, name='act', act_type=acti)\n",
    "    fc0  = mx.symbol.FullyConnected(data = act, name='fc0', num_hidden=256)\n",
    "    act0 = mx.symbol.Activation(data = fc0, name='act0', act_type=acti)\n",
    "    fc1  = mx.symbol.FullyConnected(data = act0, name='fc1', num_hidden=128)\n",
    "    act1 = mx.symbol.Activation(data = fc1, name='act1', act_type=acti)\n",
    "    fc2  = mx.symbol.FullyConnected(data = act1, name = 'fc2', num_hidden = 64)\n",
    "    act2 = mx.symbol.Activation(data = fc2, name='act2', act_type=acti)\n",
    "    fc3  = mx.symbol.FullyConnected(data = act2, name='fc3', num_hidden=32)\n",
    "    act3 = mx.symbol.Activation(data = fc3, name='act3', act_type=acti)\n",
    "    fc4  = mx.symbol.FullyConnected(data = act3, name='fc4', num_hidden=16)\n",
    "    act4 = mx.symbol.Activation(data = fc4, name='act4', act_type=acti)\n",
    "    fc5  = mx.symbol.FullyConnected(data = act4, name='fc5', num_hidden=10)\n",
    "    mlp  = mx.symbol.SoftmaxOutput(data = fc5, name = 'softmax')\n",
    "    return mlp\n",
    "```\n",
    "\n",
    "As you might already notice, we intentionally add more layers than usual, as the vanished gradient problem becomes severer as the network goes deeper.\n",
    "\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "The weight initialization also has `uniform` and `xavier`. \n",
    "\n",
    "```python\n",
    "if args.init == 'uniform':\n",
    "        init = mx.init.Uniform(0.1)\n",
    "if args.init == 'xavier':\n",
    "    init = mx.init.Xavier(factor_type=\"in\", magnitude=2.34)\n",
    "```\n",
    "\n",
    "Note that we intentionally choose a near zero setting in `uniform`. \n",
    "\n",
    "### Activation  Function\n",
    "\n",
    "We would compare two different activations, `sigmoid` and `relu`. \n",
    "\n",
    "```python\n",
    "# acti = sigmoid or relu.\n",
    "act  = mx.symbol.Activation(data = fc, name='act', act_type=acti)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging with TensorBoard and Monitor\n",
    "\n",
    "In order to monitor the weight and gradient of this network in different settings, we could use MXNet's `monitor` for logging and `TensorBoard` for visualization.\n",
    "\n",
    "### Usage\n",
    "\n",
    "Here's a code snippet from `train_model.py`:\n",
    "\n",
    "```python\n",
    "import mxnet as mx\n",
    "from tensorboard import summary\n",
    "from tensorboard import FileWriter\n",
    "\n",
    "# where to keep your TensorBoard logging file\n",
    "logdir = './logs/'\n",
    "summary_writer = FileWriter(logdir)\n",
    "\n",
    "# mx.mon.Monitor's callback \n",
    "def get_gradient(g):\n",
    "    # get flatten list\n",
    "    grad = g.asnumpy().flatten()\n",
    "    # logging using tensorboard, use histogram type.\n",
    "    s = summary.histogram('fc_backward_weight', grad)\n",
    "    summary_writer.add_summary(s)\n",
    "    return mx.nd.norm(g)/np.sqrt(g.size)\n",
    "\n",
    "mon = mx.mon.Monitor(int(args.num_examples/args.batch_size), get_gradient, pattern='fc_backward_weight')  # get the gradient passed to the first fully-connnected layer.\n",
    "\n",
    "# training\n",
    "model.fit(\n",
    "        X                  = train,\n",
    "        eval_data          = val,\n",
    "        eval_metric        = eval_metrics,\n",
    "        kvstore            = kv,\n",
    "        monitor            = mon,\n",
    "        epoch_end_callback = checkpoint)\n",
    "\n",
    "# close summary_writer\n",
    "summary_writer.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./mnist/')\n",
    "from train_mnist import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to expect?\n",
    "\n",
    "If a setting suffers from an vanish gradient problem, the gradients passed from the top should be very close to zero, and the weight of the network barely change/update. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform and Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-07 15:51:59,029 Node[0] start with arguments Namespace(batch_size=128, data_dir='mnist/', gpus=None, init='uniform', kv_store='local', load_epoch=None, lr=0.1, lr_factor=1, lr_factor_epoch=1, model_prefix=None, name='uniform_sigmoid', network='mlp', num_epochs=10, num_examples=60000, save_model_prefix=None)\n",
      "2017-01-07 15:52:02,173 Node[0] \u001b[91m[Deprecation Warning] mxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n",
      "2017-01-07 15:52:02,182 Node[0] Start training with [cpu(0), cpu(1), cpu(2), cpu(3)]\n",
      "2017-01-07 15:52:04,802 Node[0] Batch:       1 fc_backward_weight             5.1907e-07\t\n",
      "2017-01-07 15:52:04,802 Node[0] Batch:       1 fc_backward_weight             4.2085e-07\t\n",
      "2017-01-07 15:52:04,803 Node[0] Batch:       1 fc_backward_weight             4.31894e-07\t\n",
      "2017-01-07 15:52:04,804 Node[0] Batch:       1 fc_backward_weight             5.80652e-07\t\n",
      "2017-01-07 15:52:09,599 Node[0] Epoch[0] Resetting Data Iterator\n",
      "2017-01-07 15:52:09,600 Node[0] Epoch[0] Time cost=7.379\n",
      "2017-01-07 15:52:09,902 Node[0] Epoch[0] Validation-accuracy=0.105769\n",
      "2017-01-07 15:52:09,903 Node[0] Epoch[0] Validation-top_k_accuracy_5=0.509115\n",
      "2017-01-07 15:52:12,474 Node[0] Batch:     469 fc_backward_weight             5.15008e-07\t\n",
      "2017-01-07 15:52:12,475 Node[0] Batch:     469 fc_backward_weight             5.52044e-07\t\n",
      "2017-01-07 15:52:12,476 Node[0] Batch:     469 fc_backward_weight             4.48535e-07\t\n",
      "2017-01-07 15:52:12,477 Node[0] Batch:     469 fc_backward_weight             5.8659e-07\t\n",
      "2017-01-07 15:52:17,161 Node[0] Epoch[1] Resetting Data Iterator\n",
      "2017-01-07 15:52:17,162 Node[0] Epoch[1] Time cost=7.258\n",
      "2017-01-07 15:52:17,465 Node[0] Epoch[1] Validation-accuracy=0.105769\n",
      "2017-01-07 15:52:17,466 Node[0] Epoch[1] Validation-top_k_accuracy_5=0.504507\n",
      "2017-01-07 15:52:20,018 Node[0] Batch:     937 fc_backward_weight             5.96259e-07\t\n",
      "2017-01-07 15:52:20,019 Node[0] Batch:     937 fc_backward_weight             5.97974e-07\t\n",
      "2017-01-07 15:52:20,020 Node[0] Batch:     937 fc_backward_weight             4.51892e-07\t\n",
      "2017-01-07 15:52:20,021 Node[0] Batch:     937 fc_backward_weight             6.5213e-07\t\n",
      "2017-01-07 15:52:24,892 Node[0] Epoch[2] Resetting Data Iterator\n",
      "2017-01-07 15:52:24,893 Node[0] Epoch[2] Time cost=7.426\n",
      "2017-01-07 15:52:25,198 Node[0] Epoch[2] Validation-accuracy=0.105769\n",
      "2017-01-07 15:52:25,198 Node[0] Epoch[2] Validation-top_k_accuracy_5=0.510216\n",
      "2017-01-07 15:52:27,719 Node[0] Batch:    1405 fc_backward_weight             6.52871e-07\t\n",
      "2017-01-07 15:52:27,720 Node[0] Batch:    1405 fc_backward_weight             6.20821e-07\t\n",
      "2017-01-07 15:52:27,720 Node[0] Batch:    1405 fc_backward_weight             4.46476e-07\t\n",
      "2017-01-07 15:52:27,721 Node[0] Batch:    1405 fc_backward_weight             7.53641e-07\t\n",
      "2017-01-07 15:52:34,255 Node[0] Epoch[3] Resetting Data Iterator\n",
      "2017-01-07 15:52:34,256 Node[0] Epoch[3] Time cost=9.057\n",
      "2017-01-07 15:52:34,556 Node[0] Epoch[3] Validation-accuracy=0.105769\n",
      "2017-01-07 15:52:34,558 Node[0] Epoch[3] Validation-top_k_accuracy_5=0.510216\n",
      "2017-01-07 15:52:37,634 Node[0] Batch:    1873 fc_backward_weight             6.63064e-07\t\n",
      "2017-01-07 15:52:37,635 Node[0] Batch:    1873 fc_backward_weight             6.33577e-07\t\n",
      "2017-01-07 15:52:37,636 Node[0] Batch:    1873 fc_backward_weight             4.2922e-07\t\n",
      "2017-01-07 15:52:37,636 Node[0] Batch:    1873 fc_backward_weight             8.31741e-07\t\n",
      "2017-01-07 15:52:43,013 Node[0] Epoch[4] Resetting Data Iterator\n",
      "2017-01-07 15:52:43,014 Node[0] Epoch[4] Time cost=8.456\n",
      "2017-01-07 15:52:43,321 Node[0] Epoch[4] Validation-accuracy=0.103666\n",
      "2017-01-07 15:52:43,322 Node[0] Epoch[4] Validation-top_k_accuracy_5=0.509014\n",
      "2017-01-07 15:52:46,048 Node[0] Batch:    2341 fc_backward_weight             6.47525e-07\t\n",
      "2017-01-07 15:52:46,050 Node[0] Batch:    2341 fc_backward_weight             6.37593e-07\t\n",
      "2017-01-07 15:52:46,051 Node[0] Batch:    2341 fc_backward_weight             4.12299e-07\t\n",
      "2017-01-07 15:52:46,052 Node[0] Batch:    2341 fc_backward_weight             8.71203e-07\t\n",
      "2017-01-07 15:52:52,142 Node[0] Epoch[5] Resetting Data Iterator\n",
      "2017-01-07 15:52:52,143 Node[0] Epoch[5] Time cost=8.819\n",
      "2017-01-07 15:52:52,465 Node[0] Epoch[5] Validation-accuracy=0.103666\n",
      "2017-01-07 15:52:52,468 Node[0] Epoch[5] Validation-top_k_accuracy_5=0.509014\n",
      "2017-01-07 15:52:55,378 Node[0] Batch:    2809 fc_backward_weight             6.23424e-07\t\n",
      "2017-01-07 15:52:55,379 Node[0] Batch:    2809 fc_backward_weight             6.33117e-07\t\n",
      "2017-01-07 15:52:55,380 Node[0] Batch:    2809 fc_backward_weight             3.99334e-07\t\n",
      "2017-01-07 15:52:55,381 Node[0] Batch:    2809 fc_backward_weight             8.78155e-07\t\n",
      "2017-01-07 15:53:01,768 Node[0] Epoch[6] Resetting Data Iterator\n",
      "2017-01-07 15:53:01,769 Node[0] Epoch[6] Time cost=9.299\n",
      "2017-01-07 15:53:02,132 Node[0] Epoch[6] Validation-accuracy=0.107472\n",
      "2017-01-07 15:53:02,133 Node[0] Epoch[6] Validation-top_k_accuracy_5=0.509014\n",
      "2017-01-07 15:53:04,694 Node[0] Batch:    3277 fc_backward_weight             5.97921e-07\t\n",
      "2017-01-07 15:53:04,695 Node[0] Batch:    3277 fc_backward_weight             6.22105e-07\t\n",
      "2017-01-07 15:53:04,696 Node[0] Batch:    3277 fc_backward_weight             3.89208e-07\t\n",
      "2017-01-07 15:53:04,697 Node[0] Batch:    3277 fc_backward_weight             8.6379e-07\t\n",
      "2017-01-07 15:53:10,700 Node[0] Epoch[7] Resetting Data Iterator\n",
      "2017-01-07 15:53:10,702 Node[0] Epoch[7] Time cost=8.568\n",
      "2017-01-07 15:53:11,146 Node[0] Epoch[7] Validation-accuracy=0.109776\n",
      "2017-01-07 15:53:11,147 Node[0] Epoch[7] Validation-top_k_accuracy_5=0.509014\n",
      "2017-01-07 15:53:14,031 Node[0] Batch:    3745 fc_backward_weight             5.73259e-07\t\n",
      "2017-01-07 15:53:14,032 Node[0] Batch:    3745 fc_backward_weight             6.06878e-07\t\n",
      "2017-01-07 15:53:14,033 Node[0] Batch:    3745 fc_backward_weight             3.80379e-07\t\n",
      "2017-01-07 15:53:14,035 Node[0] Batch:    3745 fc_backward_weight             8.37382e-07\t\n",
      "2017-01-07 15:53:19,583 Node[0] Epoch[8] Resetting Data Iterator\n",
      "2017-01-07 15:53:19,587 Node[0] Epoch[8] Time cost=8.437\n",
      "2017-01-07 15:53:19,993 Node[0] Epoch[8] Validation-accuracy=0.105970\n",
      "2017-01-07 15:53:19,994 Node[0] Epoch[8] Validation-top_k_accuracy_5=0.512620\n",
      "2017-01-07 15:53:23,390 Node[0] Batch:    4213 fc_backward_weight             5.49988e-07\t\n",
      "2017-01-07 15:53:23,391 Node[0] Batch:    4213 fc_backward_weight             5.89305e-07\t\n",
      "2017-01-07 15:53:23,392 Node[0] Batch:    4213 fc_backward_weight             3.71941e-07\t\n",
      "2017-01-07 15:53:23,393 Node[0] Batch:    4213 fc_backward_weight             8.05085e-07\t\n",
      "2017-01-07 15:53:30,738 Node[0] Epoch[9] Resetting Data Iterator\n",
      "2017-01-07 15:53:30,739 Node[0] Epoch[9] Time cost=10.743\n",
      "2017-01-07 15:53:31,142 Node[0] Epoch[9] Validation-accuracy=0.105970\n",
      "2017-01-07 15:53:31,143 Node[0] Epoch[9] Validation-top_k_accuracy_5=0.512620\n"
     ]
    }
   ],
   "source": [
    "# Uniform and sigmoid\n",
    "args = parse_args('uniform', 'uniform_sigmoid')\n",
    "data_shape = (784, )\n",
    "net = get_mlp(\"sigmoid\")\n",
    "\n",
    "# train\n",
    "train_model.fit(args, net, get_iterator(data_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen, the metrics of `fc_backward_weight` is so close to zero, and it didn't change a lot during batchs.\n",
    "\n",
    "```\n",
    "2017-01-07 15:44:38,845 Node[0] Batch:       1 fc_backward_weight             5.1907e-07\t\n",
    "2017-01-07 15:44:38,846 Node[0] Batch:       1 fc_backward_weight             4.2085e-07\t\n",
    "2017-01-07 15:44:38,847 Node[0] Batch:       1 fc_backward_weight             4.31894e-07\t\n",
    "2017-01-07 15:44:38,848 Node[0] Batch:       1 fc_backward_weight             5.80652e-07\n",
    "\n",
    "2017-01-07 15:45:50,199 Node[0] Batch:    4213 fc_backward_weight             5.49988e-07\t\n",
    "2017-01-07 15:45:50,200 Node[0] Batch:    4213 fc_backward_weight             5.89305e-07\t\n",
    "2017-01-07 15:45:50,201 Node[0] Batch:    4213 fc_backward_weight             3.71941e-07\t\n",
    "2017-01-07 15:45:50,202 Node[0] Batch:    4213 fc_backward_weight             8.05085e-07\n",
    "```\n",
    "\n",
    "You might wonder why we have 4 different `fc_backward_weight`, cause we use 4 cpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform and ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-07 15:54:06,129 Node[0] start with arguments Namespace(batch_size=128, data_dir='mnist/', gpus=None, init='uniform', kv_store='local', load_epoch=None, lr=0.1, lr_factor=1, lr_factor_epoch=1, model_prefix=None, name='uniform_relu', network='mlp', num_epochs=10, num_examples=60000, save_model_prefix=None)\n",
      "2017-01-07 15:54:09,560 Node[0] \u001b[91m[Deprecation Warning] mxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n",
      "2017-01-07 15:54:09,568 Node[0] Start training with [cpu(0), cpu(1), cpu(2), cpu(3)]\n",
      "2017-01-07 15:54:12,286 Node[0] Batch:       1 fc_backward_weight             0.000267409\t\n",
      "2017-01-07 15:54:12,287 Node[0] Batch:       1 fc_backward_weight             0.00031988\t\n",
      "2017-01-07 15:54:12,288 Node[0] Batch:       1 fc_backward_weight             0.000306785\t\n",
      "2017-01-07 15:54:12,289 Node[0] Batch:       1 fc_backward_weight             0.000347533\t\n",
      "2017-01-07 15:54:18,112 Node[0] Epoch[0] Resetting Data Iterator\n",
      "2017-01-07 15:54:18,115 Node[0] Epoch[0] Time cost=8.501\n",
      "2017-01-07 15:54:18,490 Node[0] Epoch[0] Validation-accuracy=0.694912\n",
      "2017-01-07 15:54:18,491 Node[0] Epoch[0] Validation-top_k_accuracy_5=0.976362\n",
      "2017-01-07 15:54:21,076 Node[0] Batch:     469 fc_backward_weight             0.0527437\t\n",
      "2017-01-07 15:54:21,077 Node[0] Batch:     469 fc_backward_weight             0.0421219\t\n",
      "2017-01-07 15:54:21,078 Node[0] Batch:     469 fc_backward_weight             0.0495309\t\n",
      "2017-01-07 15:54:21,079 Node[0] Batch:     469 fc_backward_weight             0.0421051\t\n",
      "2017-01-07 15:54:25,876 Node[0] Epoch[1] Resetting Data Iterator\n",
      "2017-01-07 15:54:25,877 Node[0] Epoch[1] Time cost=7.385\n",
      "2017-01-07 15:54:26,182 Node[0] Epoch[1] Validation-accuracy=0.907652\n",
      "2017-01-07 15:54:26,183 Node[0] Epoch[1] Validation-top_k_accuracy_5=0.986679\n",
      "2017-01-07 15:54:28,738 Node[0] Batch:     937 fc_backward_weight             0.0285753\t\n",
      "2017-01-07 15:54:28,739 Node[0] Batch:     937 fc_backward_weight             0.0520748\t\n",
      "2017-01-07 15:54:28,740 Node[0] Batch:     937 fc_backward_weight             0.0807526\t\n",
      "2017-01-07 15:54:28,742 Node[0] Batch:     937 fc_backward_weight             0.0502396\t\n",
      "2017-01-07 15:54:34,071 Node[0] Epoch[2] Resetting Data Iterator\n",
      "2017-01-07 15:54:34,073 Node[0] Epoch[2] Time cost=7.890\n",
      "2017-01-07 15:54:34,404 Node[0] Epoch[2] Validation-accuracy=0.921675\n",
      "2017-01-07 15:54:34,405 Node[0] Epoch[2] Validation-top_k_accuracy_5=0.987380\n",
      "2017-01-07 15:54:36,924 Node[0] Batch:    1405 fc_backward_weight             0.0596137\t\n",
      "2017-01-07 15:54:36,925 Node[0] Batch:    1405 fc_backward_weight             0.145902\t\n",
      "2017-01-07 15:54:36,926 Node[0] Batch:    1405 fc_backward_weight             0.0783883\t\n",
      "2017-01-07 15:54:36,927 Node[0] Batch:    1405 fc_backward_weight             0.0810687\t\n",
      "2017-01-07 15:54:41,885 Node[0] Epoch[3] Resetting Data Iterator\n",
      "2017-01-07 15:54:41,886 Node[0] Epoch[3] Time cost=7.480\n",
      "2017-01-07 15:54:42,210 Node[0] Epoch[3] Validation-accuracy=0.947516\n",
      "2017-01-07 15:54:42,211 Node[0] Epoch[3] Validation-top_k_accuracy_5=0.990084\n",
      "2017-01-07 15:54:44,714 Node[0] Batch:    1873 fc_backward_weight             0.113804\t\n",
      "2017-01-07 15:54:44,715 Node[0] Batch:    1873 fc_backward_weight             0.0355092\t\n",
      "2017-01-07 15:54:44,716 Node[0] Batch:    1873 fc_backward_weight             0.0510211\t\n",
      "2017-01-07 15:54:44,716 Node[0] Batch:    1873 fc_backward_weight             0.0461469\t\n",
      "2017-01-07 15:54:49,711 Node[0] Epoch[4] Resetting Data Iterator\n",
      "2017-01-07 15:54:49,712 Node[0] Epoch[4] Time cost=7.498\n",
      "2017-01-07 15:54:50,021 Node[0] Epoch[4] Validation-accuracy=0.949319\n",
      "2017-01-07 15:54:50,023 Node[0] Epoch[4] Validation-top_k_accuracy_5=0.991587\n",
      "2017-01-07 15:54:52,664 Node[0] Batch:    2341 fc_backward_weight             0.0304884\t\n",
      "2017-01-07 15:54:52,665 Node[0] Batch:    2341 fc_backward_weight             0.0153732\t\n",
      "2017-01-07 15:54:52,666 Node[0] Batch:    2341 fc_backward_weight             0.0638052\t\n",
      "2017-01-07 15:54:52,667 Node[0] Batch:    2341 fc_backward_weight             0.0358958\t\n",
      "2017-01-07 15:54:58,352 Node[0] Epoch[5] Resetting Data Iterator\n",
      "2017-01-07 15:54:58,353 Node[0] Epoch[5] Time cost=8.330\n",
      "2017-01-07 15:54:58,663 Node[0] Epoch[5] Validation-accuracy=0.952224\n",
      "2017-01-07 15:54:58,664 Node[0] Epoch[5] Validation-top_k_accuracy_5=0.991687\n",
      "2017-01-07 15:55:01,141 Node[0] Batch:    2809 fc_backward_weight             0.180743\t\n",
      "2017-01-07 15:55:01,142 Node[0] Batch:    2809 fc_backward_weight             0.0453026\t\n",
      "2017-01-07 15:55:01,143 Node[0] Batch:    2809 fc_backward_weight             0.0212601\t\n",
      "2017-01-07 15:55:01,144 Node[0] Batch:    2809 fc_backward_weight             0.0950233\t\n",
      "2017-01-07 15:55:06,363 Node[0] Epoch[6] Resetting Data Iterator\n",
      "2017-01-07 15:55:06,364 Node[0] Epoch[6] Time cost=7.700\n",
      "2017-01-07 15:55:06,646 Node[0] Epoch[6] Validation-accuracy=0.949219\n",
      "2017-01-07 15:55:06,647 Node[0] Epoch[6] Validation-top_k_accuracy_5=0.992889\n",
      "2017-01-07 15:55:09,153 Node[0] Batch:    3277 fc_backward_weight             0.0977342\t\n",
      "2017-01-07 15:55:09,154 Node[0] Batch:    3277 fc_backward_weight             0.0354421\t\n",
      "2017-01-07 15:55:09,155 Node[0] Batch:    3277 fc_backward_weight             0.00394049\t\n",
      "2017-01-07 15:55:09,156 Node[0] Batch:    3277 fc_backward_weight             0.0402826\t\n",
      "2017-01-07 15:55:15,473 Node[0] Epoch[7] Resetting Data Iterator\n",
      "2017-01-07 15:55:15,475 Node[0] Epoch[7] Time cost=8.827\n",
      "2017-01-07 15:55:15,867 Node[0] Epoch[7] Validation-accuracy=0.956130\n",
      "2017-01-07 15:55:15,868 Node[0] Epoch[7] Validation-top_k_accuracy_5=0.993389\n",
      "2017-01-07 15:55:18,700 Node[0] Batch:    3745 fc_backward_weight             0.012503\t\n",
      "2017-01-07 15:55:18,701 Node[0] Batch:    3745 fc_backward_weight             0.064014\t\n",
      "2017-01-07 15:55:18,702 Node[0] Batch:    3745 fc_backward_weight             0.0158367\t\n",
      "2017-01-07 15:55:18,703 Node[0] Batch:    3745 fc_backward_weight             0.00945755\t\n",
      "2017-01-07 15:55:23,187 Node[0] Epoch[8] Resetting Data Iterator\n",
      "2017-01-07 15:55:23,188 Node[0] Epoch[8] Time cost=7.314\n",
      "2017-01-07 15:55:23,488 Node[0] Epoch[8] Validation-accuracy=0.957031\n",
      "2017-01-07 15:55:23,489 Node[0] Epoch[8] Validation-top_k_accuracy_5=0.992788\n",
      "2017-01-07 15:55:25,936 Node[0] Batch:    4213 fc_backward_weight             0.0226081\t\n",
      "2017-01-07 15:55:25,937 Node[0] Batch:    4213 fc_backward_weight             0.0039793\t\n",
      "2017-01-07 15:55:25,937 Node[0] Batch:    4213 fc_backward_weight             0.0306151\t\n",
      "2017-01-07 15:55:25,938 Node[0] Batch:    4213 fc_backward_weight             0.00818676\t\n",
      "2017-01-07 15:55:30,435 Node[0] Epoch[9] Resetting Data Iterator\n",
      "2017-01-07 15:55:30,436 Node[0] Epoch[9] Time cost=6.945\n",
      "2017-01-07 15:55:30,731 Node[0] Epoch[9] Validation-accuracy=0.959736\n",
      "2017-01-07 15:55:30,732 Node[0] Epoch[9] Validation-top_k_accuracy_5=0.991987\n"
     ]
    }
   ],
   "source": [
    "# Uniform and sigmoid\n",
    "args = parse_args('uniform', 'uniform_relu')\n",
    "data_shape = (784, )\n",
    "net = get_mlp(\"relu\")\n",
    "\n",
    "# train\n",
    "train_model.fit(args, net, get_iterator(data_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even we have a \"poor\" initialization, the model could still converge quickly with proper activation function. And its magnitude has significant difference.\n",
    "\n",
    "```\n",
    "2017-01-07 15:54:12,286 Node[0] Batch:       1 fc_backward_weight             0.000267409\t\n",
    "2017-01-07 15:54:12,287 Node[0] Batch:       1 fc_backward_weight             0.00031988\t\n",
    "2017-01-07 15:54:12,288 Node[0] Batch:       1 fc_backward_weight             0.000306785\t\n",
    "2017-01-07 15:54:12,289 Node[0] Batch:       1 fc_backward_weight             0.000347533\n",
    "\n",
    "2017-01-07 15:55:25,936 Node[0] Batch:    4213 fc_backward_weight             0.0226081\t\n",
    "2017-01-07 15:55:25,937 Node[0] Batch:    4213 fc_backward_weight             0.0039793\t\n",
    "2017-01-07 15:55:25,937 Node[0] Batch:    4213 fc_backward_weight             0.0306151\t\n",
    "2017-01-07 15:55:25,938 Node[0] Batch:    4213 fc_backward_weight             0.00818676\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier and Sigmoid  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-07 15:59:10,021 Node[0] start with arguments Namespace(batch_size=128, data_dir='mnist/', gpus=None, init='xavier', kv_store='local', load_epoch=None, lr=0.1, lr_factor=1, lr_factor_epoch=1, model_prefix=None, name='xavier_sigmoid', network='mlp', num_epochs=10, num_examples=60000, save_model_prefix=None)\n",
      "2017-01-07 15:59:13,291 Node[0] \u001b[91m[Deprecation Warning] mxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n",
      "2017-01-07 15:59:13,299 Node[0] Start training with [cpu(0), cpu(1), cpu(2), cpu(3)]\n",
      "2017-01-07 15:59:15,909 Node[0] Batch:       1 fc_backward_weight             9.27798e-06\t\n",
      "2017-01-07 15:59:15,909 Node[0] Batch:       1 fc_backward_weight             8.58008e-06\t\n",
      "2017-01-07 15:59:15,910 Node[0] Batch:       1 fc_backward_weight             8.96261e-06\t\n",
      "2017-01-07 15:59:15,911 Node[0] Batch:       1 fc_backward_weight             7.33611e-06\t\n",
      "2017-01-07 15:59:20,779 Node[0] Epoch[0] Resetting Data Iterator\n",
      "2017-01-07 15:59:20,780 Node[0] Epoch[0] Time cost=7.433\n",
      "2017-01-07 15:59:21,086 Node[0] Epoch[0] Validation-accuracy=0.105769\n",
      "2017-01-07 15:59:21,087 Node[0] Epoch[0] Validation-top_k_accuracy_5=0.509115\n",
      "2017-01-07 15:59:23,778 Node[0] Batch:     469 fc_backward_weight             6.76125e-06\t\n",
      "2017-01-07 15:59:23,779 Node[0] Batch:     469 fc_backward_weight             6.54805e-06\t\n",
      "2017-01-07 15:59:23,780 Node[0] Batch:     469 fc_backward_weight             6.80302e-06\t\n",
      "2017-01-07 15:59:23,782 Node[0] Batch:     469 fc_backward_weight             7.39115e-06\t\n",
      "2017-01-07 15:59:29,174 Node[0] Epoch[1] Resetting Data Iterator\n",
      "2017-01-07 15:59:29,175 Node[0] Epoch[1] Time cost=8.087\n",
      "2017-01-07 15:59:29,477 Node[0] Epoch[1] Validation-accuracy=0.105769\n",
      "2017-01-07 15:59:29,477 Node[0] Epoch[1] Validation-top_k_accuracy_5=0.504507\n",
      "2017-01-07 15:59:32,143 Node[0] Batch:     937 fc_backward_weight             5.83071e-06\t\n",
      "2017-01-07 15:59:32,144 Node[0] Batch:     937 fc_backward_weight             5.59626e-06\t\n",
      "2017-01-07 15:59:32,145 Node[0] Batch:     937 fc_backward_weight             5.776e-06\t\n",
      "2017-01-07 15:59:32,147 Node[0] Batch:     937 fc_backward_weight             6.28738e-06\t\n",
      "2017-01-07 15:59:37,783 Node[0] Epoch[2] Resetting Data Iterator\n",
      "2017-01-07 15:59:37,784 Node[0] Epoch[2] Time cost=8.305\n",
      "2017-01-07 15:59:38,085 Node[0] Epoch[2] Validation-accuracy=0.105769\n",
      "2017-01-07 15:59:38,086 Node[0] Epoch[2] Validation-top_k_accuracy_5=0.510216\n",
      "2017-01-07 15:59:41,031 Node[0] Batch:    1405 fc_backward_weight             4.951e-06\t\n",
      "2017-01-07 15:59:41,032 Node[0] Batch:    1405 fc_backward_weight             4.72836e-06\t\n",
      "2017-01-07 15:59:41,033 Node[0] Batch:    1405 fc_backward_weight             4.8514e-06\t\n",
      "2017-01-07 15:59:41,034 Node[0] Batch:    1405 fc_backward_weight             5.26915e-06\t\n",
      "2017-01-07 15:59:47,042 Node[0] Epoch[3] Resetting Data Iterator\n",
      "2017-01-07 15:59:47,043 Node[0] Epoch[3] Time cost=8.957\n",
      "2017-01-07 15:59:47,424 Node[0] Epoch[3] Validation-accuracy=0.105769\n",
      "2017-01-07 15:59:47,425 Node[0] Epoch[3] Validation-top_k_accuracy_5=0.509014\n",
      "2017-01-07 15:59:50,295 Node[0] Batch:    1873 fc_backward_weight             4.22193e-06\t\n",
      "2017-01-07 15:59:50,296 Node[0] Batch:    1873 fc_backward_weight             4.03044e-06\t\n",
      "2017-01-07 15:59:50,297 Node[0] Batch:    1873 fc_backward_weight             4.11877e-06\t\n",
      "2017-01-07 15:59:50,298 Node[0] Batch:    1873 fc_backward_weight             4.45402e-06\t\n",
      "2017-01-07 15:59:56,082 Node[0] Epoch[4] Resetting Data Iterator\n",
      "2017-01-07 15:59:56,083 Node[0] Epoch[4] Time cost=8.653\n",
      "2017-01-07 15:59:56,378 Node[0] Epoch[4] Validation-accuracy=0.105769\n",
      "2017-01-07 15:59:56,379 Node[0] Epoch[4] Validation-top_k_accuracy_5=0.509014\n",
      "2017-01-07 15:59:58,837 Node[0] Batch:    2341 fc_backward_weight             3.64564e-06\t\n",
      "2017-01-07 15:59:58,838 Node[0] Batch:    2341 fc_backward_weight             3.48901e-06\t\n",
      "2017-01-07 15:59:58,839 Node[0] Batch:    2341 fc_backward_weight             3.55765e-06\t\n",
      "2017-01-07 15:59:58,840 Node[0] Batch:    2341 fc_backward_weight             3.82692e-06\t\n",
      "2017-01-07 16:00:03,458 Node[0] Epoch[5] Resetting Data Iterator\n",
      "2017-01-07 16:00:03,459 Node[0] Epoch[5] Time cost=7.078\n",
      "2017-01-07 16:00:03,790 Node[0] Epoch[5] Validation-accuracy=0.105769\n",
      "2017-01-07 16:00:03,791 Node[0] Epoch[5] Validation-top_k_accuracy_5=0.509014\n",
      "2017-01-07 16:00:06,406 Node[0] Batch:    2809 fc_backward_weight             3.19336e-06\t\n",
      "2017-01-07 16:00:06,407 Node[0] Batch:    2809 fc_backward_weight             3.06777e-06\t\n",
      "2017-01-07 16:00:06,409 Node[0] Batch:    2809 fc_backward_weight             3.12543e-06\t\n",
      "2017-01-07 16:00:06,410 Node[0] Batch:    2809 fc_backward_weight             3.34344e-06\t\n",
      "2017-01-07 16:00:12,052 Node[0] Epoch[6] Resetting Data Iterator\n",
      "2017-01-07 16:00:12,053 Node[0] Epoch[6] Time cost=8.261\n",
      "2017-01-07 16:00:12,352 Node[0] Epoch[6] Validation-accuracy=0.107472\n",
      "2017-01-07 16:00:12,353 Node[0] Epoch[6] Validation-top_k_accuracy_5=0.509014\n",
      "2017-01-07 16:00:14,968 Node[0] Batch:    3277 fc_backward_weight             2.83478e-06\t\n",
      "2017-01-07 16:00:14,969 Node[0] Batch:    3277 fc_backward_weight             2.73443e-06\t\n",
      "2017-01-07 16:00:14,970 Node[0] Batch:    3277 fc_backward_weight             2.78607e-06\t\n",
      "2017-01-07 16:00:14,971 Node[0] Batch:    3277 fc_backward_weight             2.9644e-06\t\n",
      "2017-01-07 16:00:20,252 Node[0] Epoch[7] Resetting Data Iterator\n",
      "2017-01-07 16:00:20,253 Node[0] Epoch[7] Time cost=7.899\n",
      "2017-01-07 16:00:20,541 Node[0] Epoch[7] Validation-accuracy=0.105970\n",
      "2017-01-07 16:00:20,542 Node[0] Epoch[7] Validation-top_k_accuracy_5=0.512620\n",
      "2017-01-07 16:00:23,036 Node[0] Batch:    3745 fc_backward_weight             2.54587e-06\t\n",
      "2017-01-07 16:00:23,037 Node[0] Batch:    3745 fc_backward_weight             2.46527e-06\t\n",
      "2017-01-07 16:00:23,038 Node[0] Batch:    3745 fc_backward_weight             2.51372e-06\t\n",
      "2017-01-07 16:00:23,039 Node[0] Batch:    3745 fc_backward_weight             2.66109e-06\t\n",
      "2017-01-07 16:00:27,410 Node[0] Epoch[8] Resetting Data Iterator\n",
      "2017-01-07 16:00:27,411 Node[0] Epoch[8] Time cost=6.868\n",
      "2017-01-07 16:00:27,718 Node[0] Epoch[8] Validation-accuracy=0.105970\n",
      "2017-01-07 16:00:27,719 Node[0] Epoch[8] Validation-top_k_accuracy_5=0.512620\n",
      "2017-01-07 16:00:30,358 Node[0] Batch:    4213 fc_backward_weight             2.30903e-06\t\n",
      "2017-01-07 16:00:30,359 Node[0] Batch:    4213 fc_backward_weight             2.24373e-06\t\n",
      "2017-01-07 16:00:30,360 Node[0] Batch:    4213 fc_backward_weight             2.29058e-06\t\n",
      "2017-01-07 16:00:30,361 Node[0] Batch:    4213 fc_backward_weight             2.41351e-06\t\n",
      "2017-01-07 16:00:35,874 Node[0] Epoch[9] Resetting Data Iterator\n",
      "2017-01-07 16:00:35,875 Node[0] Epoch[9] Time cost=8.156\n",
      "2017-01-07 16:00:36,182 Node[0] Epoch[9] Validation-accuracy=0.105970\n",
      "2017-01-07 16:00:36,183 Node[0] Epoch[9] Validation-top_k_accuracy_5=0.512620\n"
     ]
    }
   ],
   "source": [
    "# Xavier and sigmoid\n",
    "args = parse_args('xavier', 'xavier_sigmoid')\n",
    "data_shape = (784, )\n",
    "net = get_mlp(\"sigmoid\")\n",
    "\n",
    "# train\n",
    "train_model.fit(args, net, get_iterator(data_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Now start using TensorBoard:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=logs/\n",
    "```\n",
    "\n",
    "![Dashboard](https://github.com/zihaolucky/tensorboard/raw/data/docs/tutorial/mnist/pic1.png)\n",
    "\n",
    "![dist](https://github.com/zihaolucky/tensorboard/raw/data/docs/tutorial/mnist/pic2.png)\n",
    "\n",
    "![hist](https://github.com/zihaolucky/tensorboard/raw/data/docs/tutorial/mnist/pic3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "You might find these materials useful:\n",
    "\n",
    "[1] [Rohan #4: The vanishing gradient problem – A Year of Artificial Intelligence](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.bojpejg3o)    \n",
    "[2] [On the difficulty of training recurrent and deep neural networks - YouTube](https://www.youtube.com/watch?v=A7poQbTrhxc)    \n",
    "[3] [What is the vanishing gradient problem? - Quora](https://www.quora.com/What-is-the-vanishing-gradient-problem)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}